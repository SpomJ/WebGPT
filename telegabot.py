# -*- coding: utf-8 -*-
"""TelegaBot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Ucet-VL3rTnfjJQRdSi7UAXQoVWg8JE
"""

import telebot
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

bot = telebot.TeleBot('6449634010:AAFMpPNmy1NEyxa45oVfjSsY_D1fDZxgQmo')


@bot.message_handler(commands=['start'])
def start(message):
    print(message)
    bot.send_message(message.chat.id,
                     'Hi, I am webGPT, your personal assistant. I possess knowledge as pretrained model and I also have an ability to send web requests about events in the world. Feel free to ask anything.')


@bot.message_handler()
def gen(message):
    input_text = message.text
    input_ids = tokenizer(input_text, return_tensors="pt")

    generation_config = GenerationConfig.from_pretrained("gpt2", num_beams=3, do_sample=True, max_new_tokens=128,
                                                         repetition_penalty=1.6)

    outputs = model.generate(**input_ids, generation_config=generation_config)
    bot.send_message(message.chat.id, text=tokenizer.decode(outputs[0]))


bot.infinity_polling()